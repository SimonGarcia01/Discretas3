{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ33rowtaSaU",
        "outputId": "c4c3c6c2-a2a7-4419-b07c-80a5e1830aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.5868085622787476\n",
            "Input sequence:\n",
            "[1. 1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
            "Expected Output sequence:\n",
            "[1. 1. 0. 0. 0. 1. 1. 0. 0. 1.]\n",
            "NTM Output sequence:\n",
            "[-0.08042132 -0.08032616 -0.08483903 -0.08474657 -0.08465458 -0.07995401\n",
            " -0.07985984 -0.08437518 -0.08428452 -0.07958482]\n",
            "==================================================\n",
            "Epoch 10, Loss: 0.5584716200828552\n",
            "Input sequence:\n",
            "[1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "Expected Output sequence:\n",
            "[1. 1. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            "NTM Output sequence:\n",
            "[0.15709254 0.16711842 0.11566921 0.11582641 0.16758935 0.16773868\n",
            " 0.16788743 0.16803558 0.16818315 0.16833013]\n",
            "==================================================\n",
            "Epoch 20, Loss: 0.21968059241771698\n",
            "Input sequence:\n",
            "[1. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n",
            "Expected Output sequence:\n",
            "[1. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n",
            "NTM Output sequence:\n",
            "[0.44373828 0.5064663  0.5067223  0.5069742  0.5072251  0.50747496\n",
            " 0.37834144 0.37858915 0.5082485  0.37908646]\n",
            "==================================================\n",
            "Epoch 30, Loss: 0.26068374514579773\n",
            "Input sequence:\n",
            "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[0.44768336 0.5772417  0.8199342  0.5778151  0.57803375 0.820768\n",
            " 0.5785777  0.57879275 0.57905585 0.5793179 ]\n",
            "==================================================\n",
            "Epoch 40, Loss: 0.11869575828313828\n",
            "Input sequence:\n",
            "[1. 1. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
            "Expected Output sequence:\n",
            "[1. 1. 1. 0. 0. 1. 0. 1. 0. 1.]\n",
            "NTM Output sequence:\n",
            "[0.6701592  0.7784962  0.7785591  0.45630038 0.45640033 0.7788654\n",
            " 0.4566     0.77906394 0.45675904 0.779261  ]\n",
            "==================================================\n",
            "Epoch 50, Loss: 0.08313217014074326\n",
            "Input sequence:\n",
            "[0. 1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 1. 1. 0. 0. 1. 1. 1. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[0.2592842  0.6930009  0.69292104 0.27054048 0.27071077 0.693161\n",
            " 0.6930806  0.6929989  0.270536   0.2707064 ]\n",
            "==================================================\n",
            "Epoch 60, Loss: 0.0452205166220665\n",
            "Input sequence:\n",
            "[0. 1. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
            "Expected Output sequence:\n",
            "[0. 1. 0. 1. 1. 1. 0. 0. 1. 0.]\n",
            "NTM Output sequence:\n",
            "[0.197316   0.74279135 0.14352466 0.7428329  0.7425903  0.7423894\n",
            " 0.1430562  0.1433768  0.74271166 0.14331357]\n",
            "==================================================\n",
            "Epoch 70, Loss: 0.007216530852019787\n",
            "Input sequence:\n",
            "[1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[0.9480195  0.09571927 0.95084774 0.9504001  0.09515851 0.09563794\n",
            " 0.09597062 0.09630188 0.09663164 0.09695993]\n",
            "==================================================\n",
            "Epoch 80, Loss: 0.0011663486948236823\n",
            "Input sequence:\n",
            "[1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
            "Expected Output sequence:\n",
            "[1. 0. 1. 0. 1. 0. 0. 1. 0. 1.]\n",
            "NTM Output sequence:\n",
            "[ 1.0395043  -0.04510171  0.9995801  -0.04516485  0.99954385 -0.04523821\n",
            " -0.04444534  0.99995905 -0.04479503  0.9999197 ]\n",
            "==================================================\n",
            "Epoch 90, Loss: 0.002253736136481166\n",
            "Input sequence:\n",
            "[1. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
            "Expected Output sequence:\n",
            "[1. 0. 1. 1. 1. 0. 1. 0. 0. 0.]\n",
            "NTM Output sequence:\n",
            "[ 1.0603287  -0.04232099  1.0498426   1.0491171   1.0486015  -0.04368805\n",
            "  1.0487449  -0.043792   -0.04295141 -0.04243333]\n",
            "==================================================\n",
            "Training complete.\n",
            "\n",
            "Final Input sequence:\n",
            "[1. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
            "\n",
            "Final Expected Output sequence:\n",
            "[1. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n",
            "\n",
            "Final NTM Output sequence:\n",
            "[ 0.99721235  1.0083928   1.0079544  -0.01990184  1.008143   -0.01993797\n",
            " -0.01914755 -0.01866213  1.0089469  -0.01900658]\n",
            "[1. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define constants\n",
        "INPUT_SIZE = 1     # Size of input data (1 for binary input)\n",
        "MEMORY_SIZE = 128  # Number of memory locations\n",
        "MEMORY_DIM = 20    # Dimensionality of each memory slot\n",
        "CONTROLLER_HIDDEN_SIZE = 100  # Size of controller hidden layer\n",
        "SEQ_LEN = 10       # Length of the input sequence\n",
        "\n",
        "# Controller network\n",
        "class Controller(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Controller, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, _ = self.rnn(x)\n",
        "        return self.fc(h[:, -1, :])\n",
        "\n",
        "# Memory module\n",
        "class Memory(nn.Module):\n",
        "    def __init__(self, memory_size, memory_dim):\n",
        "        super(Memory, self).__init__()\n",
        "        self.memory = torch.randn(memory_size, memory_dim) * 0.01\n",
        "\n",
        "    def read(self, address):\n",
        "        return torch.matmul(address.unsqueeze(0), self.memory).squeeze(0)\n",
        "\n",
        "    def write(self, address, erase_vector, add_vector):\n",
        "        address = address.view(-1, 1)\n",
        "        erase_matrix = address * erase_vector.unsqueeze(0)\n",
        "        add_matrix = address * add_vector.unsqueeze(0)\n",
        "        self.memory = self.memory * (1 - erase_matrix) + add_matrix\n",
        "\n",
        "# Read-Write head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, memory_size, memory_dim):\n",
        "        super(Head, self).__init__()\n",
        "        self.memory_size = memory_size\n",
        "        self.memory_dim = memory_dim\n",
        "        self.addressing = nn.Linear(CONTROLLER_HIDDEN_SIZE, memory_size)\n",
        "        self.erase = nn.Linear(CONTROLLER_HIDDEN_SIZE, memory_dim)\n",
        "        self.add = nn.Linear(CONTROLLER_HIDDEN_SIZE, memory_dim)\n",
        "\n",
        "    def forward(self, control_vector, memory):\n",
        "        address_weights = torch.softmax(self.addressing(control_vector), dim=-1)\n",
        "        erase_vector = torch.sigmoid(self.erase(control_vector))\n",
        "        add_vector = torch.tanh(self.add(control_vector))\n",
        "        memory.write(address_weights, erase_vector, add_vector)\n",
        "        read_data = memory.read(address_weights)\n",
        "        return read_data\n",
        "\n",
        "# Neural Turing Machine model\n",
        "class NTM(nn.Module):\n",
        "    def __init__(self, input_size, memory_size, memory_dim, controller_hidden_size):\n",
        "        super(NTM, self).__init__()\n",
        "        self.controller = Controller(input_size + memory_dim, controller_hidden_size, controller_hidden_size)\n",
        "        self.memory = Memory(memory_size, memory_dim)\n",
        "        self.head = Head(memory_size, memory_dim)\n",
        "        self.fc = nn.Linear(controller_hidden_size + memory_dim, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        memory_output = torch.zeros(batch_size, MEMORY_DIM).detach()  # Detach to avoid gradient issues\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            controller_input = torch.cat([x[:, t, :], memory_output], dim=-1)\n",
        "            control_vector = self.controller(controller_input.unsqueeze(1))\n",
        "            memory_output = self.head(control_vector, self.memory)\n",
        "            memory_output = memory_output.detach()  # Detach here to avoid retaining computation graph\n",
        "            output = self.fc(torch.cat([control_vector, memory_output], dim=-1))\n",
        "            outputs.append(output)\n",
        "\n",
        "        return torch.stack(outputs, dim=1)\n",
        "\n",
        "# Generate input and target sequences for the copying task\n",
        "def generate_copy_task_data(seq_len, batch_size=1):\n",
        "    # Create a random binary sequence\n",
        "    input_seq = torch.randint(0, 2, (batch_size, seq_len, INPUT_SIZE)).float()\n",
        "    # The target is the same as the input\n",
        "    target_seq = input_seq.clone()\n",
        "    return input_seq, target_seq\n",
        "\n",
        "# Training example\n",
        "def train_ntm():\n",
        "    ntm = NTM(INPUT_SIZE, MEMORY_SIZE, MEMORY_DIM, CONTROLLER_HIDDEN_SIZE)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(ntm.parameters(), lr=0.001)\n",
        "\n",
        "    epochs = 100\n",
        "    for epoch in range(epochs):\n",
        "        input_seq, target_seq = generate_copy_task_data(SEQ_LEN)\n",
        "        optimizer.zero_grad()  # Reset gradients at the start of each epoch\n",
        "        output_seq = ntm(input_seq)  # Forward pass\n",
        "        loss = criterion(output_seq, target_seq)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update model parameters\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "            print(\"Input sequence:\")\n",
        "            print(input_seq.squeeze().numpy())\n",
        "            print(\"Expected Output sequence:\")\n",
        "            print(target_seq.squeeze().numpy())\n",
        "            print(\"NTM Output sequence:\")\n",
        "            print(output_seq.detach().squeeze().numpy())\n",
        "            print(\"=\" * 50)\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return ntm, input_seq, target_seq\n",
        "\n",
        "# Run training and test the NTM\n",
        "ntm, input_seq, target_seq = train_ntm()\n",
        "output_seq = ntm(input_seq).detach()\n",
        "\n",
        "# Show final results\n",
        "def display_final_results(input_seq, target_seq, output_seq):\n",
        "    print(\"\\nFinal Input sequence:\")\n",
        "    print(input_seq.squeeze().numpy())\n",
        "    print(\"\\nFinal Expected Output sequence:\")\n",
        "    print(target_seq.squeeze().numpy())\n",
        "    print(\"\\nFinal NTM Output sequence:\")\n",
        "    print(output_seq.detach().squeeze().numpy())\n",
        "    print(torch.absolute(torch.round(output_seq)).detach().squeeze().numpy())\n",
        "\n",
        "display_final_results(input_seq, target_seq, output_seq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmJIn8cJR6_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}