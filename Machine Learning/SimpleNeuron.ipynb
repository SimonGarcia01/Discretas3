{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJgaq6BrL9C7"
      },
      "source": [
        "An **activation function** in a neural network determines whether a neuron should be \"activated\" (i.e., produce a strong output) or not, based on the weighted sum of its inputs.\n",
        "\n",
        "Here's what happens step by step:\n",
        "1.\tInputs come into a neuron (node), each multiplied by a weight.\n",
        "2.\tThese weighted inputs are summed, and often a **bias** is added.\n",
        "3.\tThe result is passed through an activation function, which transforms it into the neuron's output.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "output=$\\phi(w_1 x_1 +w_2 x_2 +\\dots +w_n x_n +b)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUlTSEdwKp5W",
        "outputId": "071f133d-383f-44c2-9499-bdd573f085cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y calculated 0.538181736229801\n",
            "Inputs: [ 0.5 -1.   2. ]\n",
            "Output: 0.538181736229801\n"
          ]
        }
      ],
      "source": [
        "class Neuron:\n",
        "  # Initialize weights randomly and bias to zero\n",
        "  def __init__(self, num_inputs):\n",
        "    self.weights = np.random.randn(num_inputs)\n",
        "    self.bias = 0.0\n",
        "\n",
        "  # Sigmoid activation function\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "  # Compute weighted sum of inputs + bias\n",
        "  def forward(self, inputs):\n",
        "    weighted_sum = np.dot(inputs, self.weights) + self.bias\n",
        "    # Apply activation function (sigmoid in this case)\n",
        "    y=  self.sigmoid(weighted_sum)\n",
        "    print(\"y calculated\", y)\n",
        "    return y\n",
        "\n",
        "\n",
        "\n",
        "# Create a neuron with 3 inputs\n",
        "num_inputs = 3\n",
        "neuron = Neuron(num_inputs)\n",
        "\n",
        "# Example input vector\n",
        "inputs = np.array([0.5, -1.0, 2.0])\n",
        "\n",
        "# Perform forward propagation through the neuron\n",
        "output = neuron.forward(inputs)\n",
        "print(\"Inputs:\", inputs) #[ 0.5 -1. 2. ]\n",
        "print(\"Output:\", output) #0.7808865272523784"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIqpmK9nM7Kc"
      },
      "source": [
        "# Common activation functions\n",
        "| Function       | Formula                | Output Range | Notes                                                      |\n",
        "| -------------- | ---------------------- | ------------ | ---------------------------------------------------------- |\n",
        "| **Sigmoid**    | $\\frac{1}{1 + e^{-x}}$ | (0, 1)       | Good for probabilities, but can cause vanishing gradients. |\n",
        "| **Tanh**       | $\\tanh(x)$             | (-1, 1)      | Zero-centered, but still has vanishing gradient issues.    |\n",
        "| **ReLU**       | $\\max(0, x)$           | \\[0, ∞)      | Fast and simple; most common in hidden layers.             |\n",
        "| **Leaky ReLU** | $\\max(0.01x, x)$       | (−∞, ∞)      | Fixes ReLU's \"dying neuron\" problem.                       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kCh8fcpNqI4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6-kki1iNwO-",
        "outputId": "ca378bfe-5ac5-4d73-bd2f-c17b7e067260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 1.0635\n",
            "Epoch 100, Loss: 0.0281\n",
            "Epoch 200, Loss: 0.0101\n",
            "Epoch 300, Loss: 0.0056\n",
            "Epoch 400, Loss: 0.0038\n",
            "Epoch 500, Loss: 0.0028\n",
            "Epoch 600, Loss: 0.0022\n",
            "Epoch 700, Loss: 0.0018\n",
            "Epoch 800, Loss: 0.0015\n",
            "Epoch 900, Loss: 0.0013\n",
            "\n",
            "Trained predictions:\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]]\n"
          ]
        }
      ],
      "source": [
        "# Sigmoid and ReLU activation functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_deriv(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Inputs and labels for AND logic gate\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# Random weight initialization\n",
        "np.random.seed(0)\n",
        "W1 = np.random.randn(2, 2)  # input to hidden (2 neurons)\n",
        "b1 = np.zeros((1, 2))\n",
        "W2 = np.random.randn(2, 1)  # hidden to output\n",
        "b2 = np.zeros((1, 1))\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # === Forward pass ===\n",
        "    Z1 = X @ W1 + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = A1 @ W2 + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    # === Loss (binary cross-entropy) ===\n",
        "    loss = -np.mean(y * np.log(A2 + 1e-8) + (1 - y) * np.log(1 - A2 + 1e-8))\n",
        "\n",
        "    # === Backward pass ===\n",
        "    dZ2 = A2 - y                      # derivative of loss w.r.t. Z2\n",
        "    dW2 = A1.T @ dZ2\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    dA1 = dZ2 @ W2.T\n",
        "    dZ1 = dA1 * relu_deriv(Z1)\n",
        "    dW1 = X.T @ dZ1\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    # === Update weights and biases ===\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "# === Final output ===\n",
        "print(\"\\nTrained predictions:\")\n",
        "print(np.round(A2))\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
